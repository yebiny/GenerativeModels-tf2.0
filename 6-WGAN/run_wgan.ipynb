{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from wgan import *\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from getData import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "x, y  = get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2DTran (None, 7, 7, 128)         409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2D)    (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2D)    (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,275,521\n",
      "Trainable params: 1,262,593\n",
      "Non-trainable params: 12,928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator( 100\n",
    "                           , generator_initial_dense_layer_size = (7, 7, 128)\n",
    "                           , generator_upsample = [1, 2, 2]\n",
    "                           , generator_conv_filters = [128,64,1]\n",
    "                           , generator_conv_kernel_size = [5,5,5]\n",
    "                           , generator_conv_strides = [1,1,1]\n",
    "                           , generator_batch_norm_momentum = 0.8\n",
    "                           , generator_dropout_rate = None\n",
    "                           , generator_weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "                           )\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Critic\n",
    "\n",
    "* EM distance 이용해 확률 분포 측정\n",
    "* 기존 KL divergence 보다 더 좋은 결과를 낼 수 있다. \n",
    "* 이를 이용해 training 시 나타나는 G-D 간의 rl balance를 덜 신경써도 무방\n",
    "* GAN에서 일반적으로 발생하는 mode dropping 해결 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = build_critic((28,28,1)\n",
    "                    , critic_conv_filters = [32,64,128,128]\n",
    "                    , critic_conv_kernel_size = [5,5,5,5]\n",
    "                    , critic_conv_strides = [2,2,2,1]\n",
    "                    , critic_batch_norm_momentum = None\n",
    "                    , critic_activation = 'leaky_relu'\n",
    "                    , critic_dropout_rate = None\n",
    "                    , critic_weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "                    )\n",
    "#critic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOLDER='./results/mnist_2'\n",
    "wgan = WGAN(generator, critic, RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_input (InputLayer)     [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model_8 (Model)              (None, 28, 28, 1)         1275521   \n",
      "_________________________________________________________________\n",
      "model_9 (Model)              (None, 1)                 668801    \n",
      "=================================================================\n",
      "Total params: 1,275,521\n",
      "Trainable params: 1,262,593\n",
      "Non-trainable params: 12,928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wgan.compile( optimizer = 'rmsprop'\n",
    "            , generator_lr=0.00005\n",
    "            , critic_lr=0.00005\n",
    "            )\n",
    "wgan.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load weights ( if youn need )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.load_weights('results/mnist_1/weights/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (-0.007)(R -0.210, F 0.196)]  [G loss: -0.161] \n",
      "1 [D loss: (-0.008)(R -0.208, F 0.193)]  [G loss: -0.167] \n",
      "2 [D loss: (-0.006)(R -0.216, F 0.203)]  [G loss: -0.178] \n",
      "3 [D loss: (-0.002)(R -0.215, F 0.211)]  [G loss: -0.182] \n",
      "4 [D loss: (-0.002)(R -0.213, F 0.208)]  [G loss: -0.182] \n",
      "5 [D loss: (-0.003)(R -0.216, F 0.210)]  [G loss: -0.184] \n",
      "6 [D loss: (-0.003)(R -0.220, F 0.213)]  [G loss: -0.188] \n",
      "7 [D loss: (-0.003)(R -0.220, F 0.215)]  [G loss: -0.188] \n",
      "8 [D loss: (-0.005)(R -0.222, F 0.212)]  [G loss: -0.185] \n",
      "9 [D loss: (-0.003)(R -0.218, F 0.212)]  [G loss: -0.187] \n",
      "10 [D loss: (-0.002)(R -0.215, F 0.211)]  [G loss: -0.184] \n",
      "11 [D loss: (-0.001)(R -0.218, F 0.217)]  [G loss: -0.184] \n",
      "12 [D loss: (-0.002)(R -0.216, F 0.212)]  [G loss: -0.187] \n",
      "13 [D loss: (-0.002)(R -0.214, F 0.210)]  [G loss: -0.179] \n",
      "14 [D loss: (-0.000)(R -0.208, F 0.207)]  [G loss: -0.178] \n",
      "15 [D loss: (0.001)(R -0.200, F 0.202)]  [G loss: -0.175] \n",
      "16 [D loss: (-0.001)(R -0.204, F 0.203)]  [G loss: -0.176] \n",
      "17 [D loss: (-0.001)(R -0.200, F 0.197)]  [G loss: -0.170] \n",
      "18 [D loss: (-0.000)(R -0.197, F 0.196)]  [G loss: -0.170] \n",
      "19 [D loss: (0.001)(R -0.192, F 0.193)]  [G loss: -0.165] \n",
      "20 [D loss: (0.001)(R -0.188, F 0.189)]  [G loss: -0.160] \n",
      "21 [D loss: (-0.001)(R -0.183, F 0.181)]  [G loss: -0.160] \n",
      "22 [D loss: (-0.003)(R -0.185, F 0.179)]  [G loss: -0.155] \n",
      "23 [D loss: (-0.002)(R -0.183, F 0.179)]  [G loss: -0.155] \n",
      "24 [D loss: (-0.002)(R -0.177, F 0.174)]  [G loss: -0.149] \n",
      "25 [D loss: (-0.004)(R -0.176, F 0.167)]  [G loss: -0.145] \n",
      "26 [D loss: (-0.004)(R -0.171, F 0.163)]  [G loss: -0.140] \n",
      "27 [D loss: (-0.005)(R -0.171, F 0.162)]  [G loss: -0.140] \n",
      "28 [D loss: (-0.006)(R -0.169, F 0.158)]  [G loss: -0.136] \n",
      "29 [D loss: (-0.006)(R -0.168, F 0.157)]  [G loss: -0.130] \n",
      "30 [D loss: (-0.007)(R -0.167, F 0.152)]  [G loss: -0.128] \n",
      "31 [D loss: (-0.008)(R -0.166, F 0.149)]  [G loss: -0.128] \n",
      "32 [D loss: (-0.008)(R -0.165, F 0.149)]  [G loss: -0.125] \n",
      "33 [D loss: (-0.011)(R -0.167, F 0.144)]  [G loss: -0.125] \n",
      "34 [D loss: (-0.009)(R -0.162, F 0.144)]  [G loss: -0.125] \n",
      "35 [D loss: (-0.012)(R -0.168, F 0.144)]  [G loss: -0.125] \n",
      "36 [D loss: (-0.010)(R -0.168, F 0.147)]  [G loss: -0.122] \n",
      "37 [D loss: (-0.011)(R -0.163, F 0.142)]  [G loss: -0.122] \n",
      "38 [D loss: (-0.010)(R -0.166, F 0.145)]  [G loss: -0.121] \n",
      "39 [D loss: (-0.013)(R -0.166, F 0.141)]  [G loss: -0.118] \n",
      "40 [D loss: (-0.010)(R -0.165, F 0.144)]  [G loss: -0.121] \n",
      "41 [D loss: (-0.012)(R -0.167, F 0.142)]  [G loss: -0.122] \n",
      "42 [D loss: (-0.015)(R -0.175, F 0.146)]  [G loss: -0.129] \n",
      "43 [D loss: (-0.012)(R -0.176, F 0.151)]  [G loss: -0.128] \n",
      "44 [D loss: (-0.014)(R -0.173, F 0.145)]  [G loss: -0.125] \n",
      "45 [D loss: (-0.013)(R -0.179, F 0.153)]  [G loss: -0.128] \n",
      "46 [D loss: (-0.020)(R -0.193, F 0.154)]  [G loss: -0.134] \n",
      "47 [D loss: (-0.016)(R -0.189, F 0.157)]  [G loss: -0.134] \n",
      "48 [D loss: (-0.014)(R -0.181, F 0.154)]  [G loss: -0.129] \n",
      "49 [D loss: (-0.015)(R -0.184, F 0.154)]  [G loss: -0.139] \n",
      "50 [D loss: (-0.009)(R -0.186, F 0.168)]  [G loss: -0.134] \n",
      "51 [D loss: (-0.016)(R -0.196, F 0.165)]  [G loss: -0.144] \n",
      "52 [D loss: (-0.016)(R -0.196, F 0.164)]  [G loss: -0.141] \n",
      "53 [D loss: (-0.016)(R -0.195, F 0.163)]  [G loss: -0.140] \n",
      "54 [D loss: (-0.011)(R -0.188, F 0.166)]  [G loss: -0.140] \n",
      "55 [D loss: (-0.015)(R -0.195, F 0.166)]  [G loss: -0.143] \n",
      "56 [D loss: (-0.009)(R -0.191, F 0.173)]  [G loss: -0.143] \n",
      "57 [D loss: (-0.014)(R -0.194, F 0.166)]  [G loss: -0.144] \n",
      "58 [D loss: (-0.015)(R -0.198, F 0.169)]  [G loss: -0.149] \n",
      "59 [D loss: (-0.005)(R -0.182, F 0.172)]  [G loss: -0.142] \n",
      "60 [D loss: (-0.010)(R -0.188, F 0.168)]  [G loss: -0.146] \n",
      "61 [D loss: (-0.007)(R -0.179, F 0.165)]  [G loss: -0.149] \n",
      "62 [D loss: (-0.012)(R -0.192, F 0.169)]  [G loss: -0.142] \n",
      "63 [D loss: (-0.009)(R -0.182, F 0.163)]  [G loss: -0.140] \n",
      "64 [D loss: (-0.003)(R -0.176, F 0.170)]  [G loss: -0.140] \n",
      "65 [D loss: (-0.013)(R -0.193, F 0.166)]  [G loss: -0.139] \n",
      "66 [D loss: (-0.011)(R -0.183, F 0.161)]  [G loss: -0.143] \n",
      "67 [D loss: (-0.006)(R -0.181, F 0.170)]  [G loss: -0.142] \n",
      "68 [D loss: (-0.008)(R -0.178, F 0.163)]  [G loss: -0.141] \n",
      "69 [D loss: (-0.006)(R -0.175, F 0.164)]  [G loss: -0.139] \n",
      "70 [D loss: (-0.007)(R -0.177, F 0.163)]  [G loss: -0.131] \n",
      "71 [D loss: (0.001)(R -0.159, F 0.161)]  [G loss: -0.130] \n",
      "72 [D loss: (-0.002)(R -0.162, F 0.158)]  [G loss: -0.131] \n",
      "73 [D loss: (-0.002)(R -0.163, F 0.159)]  [G loss: -0.131] \n",
      "74 [D loss: (-0.002)(R -0.155, F 0.151)]  [G loss: -0.123] \n",
      "75 [D loss: (-0.007)(R -0.154, F 0.141)]  [G loss: -0.122] \n",
      "76 [D loss: (-0.002)(R -0.155, F 0.150)]  [G loss: -0.122] \n",
      "77 [D loss: (-0.003)(R -0.148, F 0.141)]  [G loss: -0.120] \n",
      "78 [D loss: (-0.006)(R -0.152, F 0.141)]  [G loss: -0.116] \n",
      "79 [D loss: (-0.004)(R -0.144, F 0.137)]  [G loss: -0.114] \n",
      "80 [D loss: (-0.003)(R -0.145, F 0.138)]  [G loss: -0.118] \n",
      "81 [D loss: (-0.005)(R -0.146, F 0.137)]  [G loss: -0.112] \n",
      "82 [D loss: (-0.006)(R -0.148, F 0.136)]  [G loss: -0.110] \n",
      "83 [D loss: (-0.005)(R -0.137, F 0.127)]  [G loss: -0.106] \n",
      "84 [D loss: (-0.009)(R -0.146, F 0.128)]  [G loss: -0.107] \n",
      "85 [D loss: (-0.008)(R -0.147, F 0.132)]  [G loss: -0.108] \n",
      "86 [D loss: (-0.011)(R -0.150, F 0.128)]  [G loss: -0.108] \n",
      "87 [D loss: (-0.009)(R -0.146, F 0.127)]  [G loss: -0.107] \n",
      "88 [D loss: (-0.005)(R -0.140, F 0.130)]  [G loss: -0.103] \n",
      "89 [D loss: (-0.011)(R -0.152, F 0.131)]  [G loss: -0.104] \n",
      "90 [D loss: (-0.009)(R -0.148, F 0.130)]  [G loss: -0.106] \n",
      "91 [D loss: (-0.010)(R -0.149, F 0.130)]  [G loss: -0.105] \n",
      "92 [D loss: (-0.014)(R -0.153, F 0.126)]  [G loss: -0.100] \n",
      "93 [D loss: (-0.013)(R -0.154, F 0.128)]  [G loss: -0.103] \n",
      "94 [D loss: (-0.011)(R -0.144, F 0.123)]  [G loss: -0.107] \n",
      "95 [D loss: (-0.012)(R -0.152, F 0.128)]  [G loss: -0.104] \n",
      "96 [D loss: (-0.010)(R -0.148, F 0.128)]  [G loss: -0.103] \n",
      "97 [D loss: (-0.011)(R -0.147, F 0.124)]  [G loss: -0.101] \n",
      "98 [D loss: (-0.014)(R -0.154, F 0.127)]  [G loss: -0.101] \n",
      "99 [D loss: (-0.015)(R -0.155, F 0.125)]  [G loss: -0.101] \n",
      "100 [D loss: (-0.015)(R -0.154, F 0.124)]  [G loss: -0.104] \n",
      "101 [D loss: (-0.014)(R -0.153, F 0.125)]  [G loss: -0.096] \n",
      "102 [D loss: (-0.019)(R -0.165, F 0.126)]  [G loss: -0.099] \n",
      "103 [D loss: (-0.019)(R -0.163, F 0.124)]  [G loss: -0.107] \n",
      "104 [D loss: (-0.020)(R -0.167, F 0.126)]  [G loss: -0.097] \n",
      "105 [D loss: (-0.019)(R -0.166, F 0.129)]  [G loss: -0.105] \n",
      "106 [D loss: (-0.020)(R -0.169, F 0.129)]  [G loss: -0.103] \n",
      "107 [D loss: (-0.024)(R -0.173, F 0.125)]  [G loss: -0.105] \n",
      "108 [D loss: (-0.019)(R -0.172, F 0.133)]  [G loss: -0.102] \n",
      "109 [D loss: (-0.027)(R -0.185, F 0.130)]  [G loss: -0.105] \n",
      "110 [D loss: (-0.026)(R -0.185, F 0.134)]  [G loss: -0.114] \n",
      "111 [D loss: (-0.021)(R -0.177, F 0.136)]  [G loss: -0.106] \n",
      "112 [D loss: (-0.022)(R -0.187, F 0.144)]  [G loss: -0.108] \n",
      "113 [D loss: (-0.013)(R -0.167, F 0.141)]  [G loss: -0.114] \n",
      "114 [D loss: (-0.021)(R -0.180, F 0.139)]  [G loss: -0.115] \n",
      "115 [D loss: (-0.020)(R -0.182, F 0.141)]  [G loss: -0.116] \n",
      "116 [D loss: (-0.020)(R -0.190, F 0.150)]  [G loss: -0.117] \n",
      "117 [D loss: (-0.021)(R -0.181, F 0.139)]  [G loss: -0.104] \n",
      "118 [D loss: (-0.021)(R -0.187, F 0.144)]  [G loss: -0.106] \n",
      "119 [D loss: (-0.025)(R -0.192, F 0.142)]  [G loss: -0.124] \n",
      "120 [D loss: (-0.023)(R -0.192, F 0.145)]  [G loss: -0.112] \n",
      "121 [D loss: (-0.023)(R -0.188, F 0.142)]  [G loss: -0.111] \n",
      "122 [D loss: (-0.022)(R -0.186, F 0.142)]  [G loss: -0.109] \n",
      "123 [D loss: (-0.025)(R -0.194, F 0.144)]  [G loss: -0.119] \n",
      "124 [D loss: (-0.027)(R -0.200, F 0.147)]  [G loss: -0.114] \n",
      "125 [D loss: (-0.028)(R -0.201, F 0.146)]  [G loss: -0.116] \n",
      "126 [D loss: (-0.018)(R -0.191, F 0.156)]  [G loss: -0.121] \n",
      "127 [D loss: (-0.022)(R -0.197, F 0.154)]  [G loss: -0.119] \n",
      "128 [D loss: (-0.031)(R -0.214, F 0.153)]  [G loss: -0.128] \n",
      "129 [D loss: (-0.027)(R -0.200, F 0.147)]  [G loss: -0.124] \n",
      "130 [D loss: (-0.027)(R -0.212, F 0.157)]  [G loss: -0.128] \n",
      "131 [D loss: (-0.025)(R -0.208, F 0.159)]  [G loss: -0.121] \n",
      "132 [D loss: (-0.033)(R -0.225, F 0.160)]  [G loss: -0.121] \n",
      "133 [D loss: (-0.029)(R -0.223, F 0.165)]  [G loss: -0.127] \n",
      "134 [D loss: (-0.028)(R -0.220, F 0.163)]  [G loss: -0.125] \n",
      "135 [D loss: (-0.033)(R -0.227, F 0.161)]  [G loss: -0.125] \n",
      "136 [D loss: (-0.034)(R -0.230, F 0.161)]  [G loss: -0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 [D loss: (-0.033)(R -0.227, F 0.160)]  [G loss: -0.125] \n",
      "138 [D loss: (-0.039)(R -0.239, F 0.162)]  [G loss: -0.130] \n",
      "139 [D loss: (-0.032)(R -0.224, F 0.160)]  [G loss: -0.117] \n",
      "140 [D loss: (-0.034)(R -0.233, F 0.165)]  [G loss: -0.131] \n",
      "141 [D loss: (-0.044)(R -0.254, F 0.166)]  [G loss: -0.116] \n",
      "142 [D loss: (-0.035)(R -0.227, F 0.158)]  [G loss: -0.132] \n",
      "143 [D loss: (-0.040)(R -0.246, F 0.167)]  [G loss: -0.131] \n",
      "144 [D loss: (-0.040)(R -0.247, F 0.167)]  [G loss: -0.126] \n",
      "145 [D loss: (-0.039)(R -0.242, F 0.164)]  [G loss: -0.129] \n",
      "146 [D loss: (-0.043)(R -0.254, F 0.168)]  [G loss: -0.124] \n",
      "147 [D loss: (-0.046)(R -0.257, F 0.165)]  [G loss: -0.134] \n",
      "148 [D loss: (-0.050)(R -0.269, F 0.170)]  [G loss: -0.136] \n",
      "149 [D loss: (-0.038)(R -0.250, F 0.175)]  [G loss: -0.144] \n",
      "150 [D loss: (-0.053)(R -0.263, F 0.156)]  [G loss: -0.131] \n",
      "151 [D loss: (-0.058)(R -0.299, F 0.182)]  [G loss: -0.146] \n",
      "152 [D loss: (-0.039)(R -0.260, F 0.181)]  [G loss: -0.128] \n",
      "153 [D loss: (-0.038)(R -0.255, F 0.179)]  [G loss: -0.141] \n",
      "154 [D loss: (-0.043)(R -0.269, F 0.182)]  [G loss: -0.143] \n",
      "155 [D loss: (-0.057)(R -0.290, F 0.176)]  [G loss: -0.148] \n",
      "156 [D loss: (-0.031)(R -0.264, F 0.202)]  [G loss: -0.159] \n",
      "157 [D loss: (-0.055)(R -0.300, F 0.189)]  [G loss: -0.153] \n",
      "158 [D loss: (-0.034)(R -0.262, F 0.194)]  [G loss: -0.160] \n",
      "159 [D loss: (-0.036)(R -0.280, F 0.208)]  [G loss: -0.155] \n",
      "160 [D loss: (-0.037)(R -0.288, F 0.215)]  [G loss: -0.162] \n",
      "161 [D loss: (-0.044)(R -0.287, F 0.199)]  [G loss: -0.159] \n",
      "162 [D loss: (-0.038)(R -0.297, F 0.222)]  [G loss: -0.157] \n",
      "163 [D loss: (-0.033)(R -0.283, F 0.218)]  [G loss: -0.174] \n",
      "164 [D loss: (-0.034)(R -0.283, F 0.214)]  [G loss: -0.171] \n",
      "165 [D loss: (-0.031)(R -0.287, F 0.225)]  [G loss: -0.169] \n",
      "166 [D loss: (-0.030)(R -0.273, F 0.213)]  [G loss: -0.172] \n",
      "167 [D loss: (-0.040)(R -0.276, F 0.196)]  [G loss: -0.166] \n",
      "168 [D loss: (-0.038)(R -0.302, F 0.225)]  [G loss: -0.177] \n",
      "169 [D loss: (-0.039)(R -0.303, F 0.226)]  [G loss: -0.182] \n",
      "170 [D loss: (-0.027)(R -0.273, F 0.220)]  [G loss: -0.192] \n",
      "171 [D loss: (-0.019)(R -0.262, F 0.224)]  [G loss: -0.189] \n",
      "172 [D loss: (-0.042)(R -0.300, F 0.217)]  [G loss: -0.181] \n",
      "173 [D loss: (-0.038)(R -0.304, F 0.227)]  [G loss: -0.192] \n",
      "174 [D loss: (-0.038)(R -0.300, F 0.223)]  [G loss: -0.179] \n",
      "175 [D loss: (-0.039)(R -0.292, F 0.214)]  [G loss: -0.167] \n",
      "176 [D loss: (-0.039)(R -0.307, F 0.230)]  [G loss: -0.196] \n",
      "177 [D loss: (-0.049)(R -0.326, F 0.228)]  [G loss: -0.196] \n",
      "178 [D loss: (-0.035)(R -0.319, F 0.249)]  [G loss: -0.190] \n",
      "179 [D loss: (-0.046)(R -0.330, F 0.238)]  [G loss: -0.192] \n",
      "180 [D loss: (-0.032)(R -0.309, F 0.245)]  [G loss: -0.188] \n",
      "181 [D loss: (-0.037)(R -0.320, F 0.245)]  [G loss: -0.196] \n",
      "182 [D loss: (-0.043)(R -0.339, F 0.254)]  [G loss: -0.207] \n",
      "183 [D loss: (-0.025)(R -0.309, F 0.260)]  [G loss: -0.182] \n",
      "184 [D loss: (-0.040)(R -0.337, F 0.257)]  [G loss: -0.203] \n",
      "185 [D loss: (-0.037)(R -0.317, F 0.244)]  [G loss: -0.202] \n",
      "186 [D loss: (-0.043)(R -0.328, F 0.241)]  [G loss: -0.203] \n",
      "187 [D loss: (-0.043)(R -0.344, F 0.258)]  [G loss: -0.194] \n",
      "188 [D loss: (-0.040)(R -0.331, F 0.251)]  [G loss: -0.201] \n",
      "189 [D loss: (-0.032)(R -0.329, F 0.265)]  [G loss: -0.212] \n",
      "190 [D loss: (-0.048)(R -0.345, F 0.249)]  [G loss: -0.195] \n",
      "191 [D loss: (-0.055)(R -0.365, F 0.255)]  [G loss: -0.207] \n",
      "192 [D loss: (-0.026)(R -0.297, F 0.246)]  [G loss: -0.186] \n",
      "193 [D loss: (-0.046)(R -0.335, F 0.242)]  [G loss: -0.199] \n",
      "194 [D loss: (-0.035)(R -0.332, F 0.263)]  [G loss: -0.199] \n",
      "195 [D loss: (-0.039)(R -0.339, F 0.261)]  [G loss: -0.216] \n",
      "196 [D loss: (-0.034)(R -0.326, F 0.258)]  [G loss: -0.202] \n",
      "197 [D loss: (-0.041)(R -0.341, F 0.260)]  [G loss: -0.220] \n",
      "198 [D loss: (-0.043)(R -0.329, F 0.244)]  [G loss: -0.195] \n",
      "199 [D loss: (-0.084)(R -0.377, F 0.209)]  [G loss: -0.220] \n",
      "200 [D loss: (-0.018)(R -0.323, F 0.288)]  [G loss: -0.194] \n",
      "201 [D loss: (-0.034)(R -0.328, F 0.260)]  [G loss: -0.220] \n",
      "202 [D loss: (-0.057)(R -0.375, F 0.262)]  [G loss: -0.223] \n",
      "203 [D loss: (-0.044)(R -0.355, F 0.267)]  [G loss: -0.220] \n",
      "204 [D loss: (-0.033)(R -0.329, F 0.263)]  [G loss: -0.221] \n",
      "205 [D loss: (-0.038)(R -0.342, F 0.265)]  [G loss: -0.216] \n",
      "206 [D loss: (-0.027)(R -0.318, F 0.264)]  [G loss: -0.188] \n",
      "207 [D loss: (-0.043)(R -0.316, F 0.231)]  [G loss: -0.223] \n",
      "208 [D loss: (-0.041)(R -0.335, F 0.252)]  [G loss: -0.190] \n",
      "209 [D loss: (-0.034)(R -0.328, F 0.260)]  [G loss: -0.205] \n",
      "210 [D loss: (-0.051)(R -0.351, F 0.249)]  [G loss: -0.186] \n",
      "211 [D loss: (-0.041)(R -0.336, F 0.254)]  [G loss: -0.212] \n",
      "212 [D loss: (-0.035)(R -0.346, F 0.277)]  [G loss: -0.213] \n",
      "213 [D loss: (-0.037)(R -0.334, F 0.260)]  [G loss: -0.219] \n",
      "214 [D loss: (-0.043)(R -0.341, F 0.255)]  [G loss: -0.199] \n",
      "215 [D loss: (-0.070)(R -0.349, F 0.209)]  [G loss: -0.206] \n",
      "216 [D loss: (-0.044)(R -0.323, F 0.234)]  [G loss: -0.180] \n",
      "217 [D loss: (-0.040)(R -0.327, F 0.247)]  [G loss: -0.198] \n",
      "218 [D loss: (-0.035)(R -0.318, F 0.247)]  [G loss: -0.201] \n",
      "219 [D loss: (-0.048)(R -0.333, F 0.236)]  [G loss: -0.195] \n",
      "220 [D loss: (-0.038)(R -0.295, F 0.218)]  [G loss: -0.194] \n",
      "221 [D loss: (-0.025)(R -0.307, F 0.257)]  [G loss: -0.170] \n",
      "222 [D loss: (-0.038)(R -0.305, F 0.228)]  [G loss: -0.171] \n",
      "223 [D loss: (-0.048)(R -0.325, F 0.229)]  [G loss: -0.170] \n",
      "224 [D loss: (-0.057)(R -0.330, F 0.215)]  [G loss: -0.162] \n",
      "225 [D loss: (-0.047)(R -0.301, F 0.208)]  [G loss: -0.161] \n",
      "226 [D loss: (-0.046)(R -0.313, F 0.222)]  [G loss: -0.153] \n",
      "227 [D loss: (-0.056)(R -0.326, F 0.215)]  [G loss: -0.153] \n",
      "228 [D loss: (-0.060)(R -0.318, F 0.199)]  [G loss: -0.162] \n",
      "229 [D loss: (-0.056)(R -0.319, F 0.208)]  [G loss: -0.160] \n",
      "230 [D loss: (-0.060)(R -0.311, F 0.192)]  [G loss: -0.142] \n",
      "231 [D loss: (-0.072)(R -0.314, F 0.170)]  [G loss: -0.166] \n",
      "232 [D loss: (-0.054)(R -0.324, F 0.216)]  [G loss: -0.139] \n",
      "233 [D loss: (-0.051)(R -0.328, F 0.227)]  [G loss: -0.180] \n",
      "234 [D loss: (-0.054)(R -0.316, F 0.208)]  [G loss: -0.147] \n",
      "235 [D loss: (-0.045)(R -0.295, F 0.205)]  [G loss: -0.160] \n",
      "236 [D loss: (-0.048)(R -0.323, F 0.227)]  [G loss: -0.160] \n",
      "237 [D loss: (-0.046)(R -0.309, F 0.217)]  [G loss: -0.137] \n",
      "238 [D loss: (-0.068)(R -0.336, F 0.199)]  [G loss: -0.147] \n",
      "239 [D loss: (-0.057)(R -0.318, F 0.204)]  [G loss: -0.145] \n",
      "240 [D loss: (-0.065)(R -0.310, F 0.181)]  [G loss: -0.124] \n",
      "241 [D loss: (-0.096)(R -0.367, F 0.176)]  [G loss: -0.142] \n",
      "242 [D loss: (-0.069)(R -0.318, F 0.180)]  [G loss: -0.127] \n",
      "243 [D loss: (-0.061)(R -0.321, F 0.199)]  [G loss: -0.134] \n",
      "244 [D loss: (-0.060)(R -0.323, F 0.203)]  [G loss: -0.150] \n",
      "245 [D loss: (-0.081)(R -0.368, F 0.206)]  [G loss: -0.142] \n",
      "246 [D loss: (-0.060)(R -0.347, F 0.227)]  [G loss: -0.140] \n",
      "247 [D loss: (-0.066)(R -0.345, F 0.212)]  [G loss: -0.133] \n",
      "248 [D loss: (-0.069)(R -0.326, F 0.188)]  [G loss: -0.153] \n",
      "249 [D loss: (-0.064)(R -0.323, F 0.195)]  [G loss: -0.117] \n",
      "250 [D loss: (-0.063)(R -0.305, F 0.179)]  [G loss: -0.153] \n",
      "251 [D loss: (-0.077)(R -0.338, F 0.185)]  [G loss: -0.133] \n",
      "252 [D loss: (-0.057)(R -0.355, F 0.241)]  [G loss: -0.145] \n",
      "253 [D loss: (-0.067)(R -0.342, F 0.208)]  [G loss: -0.149] \n",
      "254 [D loss: (-0.053)(R -0.336, F 0.231)]  [G loss: -0.159] \n",
      "255 [D loss: (-0.071)(R -0.346, F 0.204)]  [G loss: -0.135] \n",
      "256 [D loss: (-0.068)(R -0.345, F 0.209)]  [G loss: -0.149] \n",
      "257 [D loss: (-0.071)(R -0.346, F 0.204)]  [G loss: -0.163] \n",
      "258 [D loss: (-0.075)(R -0.360, F 0.211)]  [G loss: -0.144] \n",
      "259 [D loss: (-0.068)(R -0.358, F 0.221)]  [G loss: -0.149] \n",
      "260 [D loss: (-0.069)(R -0.358, F 0.220)]  [G loss: -0.146] \n",
      "261 [D loss: (-0.081)(R -0.386, F 0.223)]  [G loss: -0.154] \n",
      "262 [D loss: (-0.061)(R -0.333, F 0.211)]  [G loss: -0.152] \n",
      "263 [D loss: (-0.072)(R -0.386, F 0.242)]  [G loss: -0.152] \n",
      "264 [D loss: (-0.100)(R -0.383, F 0.182)]  [G loss: -0.170] \n",
      "265 [D loss: (-0.090)(R -0.357, F 0.177)]  [G loss: -0.113] \n",
      "266 [D loss: (-0.048)(R -0.336, F 0.240)]  [G loss: -0.146] \n",
      "267 [D loss: (-0.073)(R -0.374, F 0.228)]  [G loss: -0.183] \n",
      "268 [D loss: (-0.085)(R -0.414, F 0.243)]  [G loss: -0.159] \n",
      "269 [D loss: (-0.087)(R -0.385, F 0.211)]  [G loss: -0.148] \n",
      "270 [D loss: (-0.060)(R -0.357, F 0.237)]  [G loss: -0.168] \n",
      "271 [D loss: (-0.052)(R -0.358, F 0.255)]  [G loss: -0.153] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 [D loss: (-0.068)(R -0.360, F 0.224)]  [G loss: -0.164] \n",
      "273 [D loss: (-0.091)(R -0.404, F 0.222)]  [G loss: -0.141] \n",
      "274 [D loss: (-0.072)(R -0.382, F 0.238)]  [G loss: -0.153] \n",
      "275 [D loss: (-0.079)(R -0.376, F 0.217)]  [G loss: -0.176] \n",
      "276 [D loss: (-0.080)(R -0.385, F 0.225)]  [G loss: -0.152] \n",
      "277 [D loss: (-0.077)(R -0.388, F 0.234)]  [G loss: -0.151] \n",
      "278 [D loss: (-0.081)(R -0.384, F 0.221)]  [G loss: -0.143] \n",
      "279 [D loss: (-0.083)(R -0.345, F 0.178)]  [G loss: -0.145] \n",
      "280 [D loss: (-0.077)(R -0.408, F 0.254)]  [G loss: -0.169] \n",
      "281 [D loss: (-0.074)(R -0.388, F 0.240)]  [G loss: -0.171] \n",
      "282 [D loss: (-0.061)(R -0.369, F 0.246)]  [G loss: -0.159] \n",
      "283 [D loss: (-0.071)(R -0.375, F 0.233)]  [G loss: -0.147] \n",
      "284 [D loss: (-0.058)(R -0.377, F 0.261)]  [G loss: -0.161] \n",
      "285 [D loss: (-0.078)(R -0.414, F 0.257)]  [G loss: -0.159] \n",
      "286 [D loss: (-0.073)(R -0.387, F 0.241)]  [G loss: -0.185] \n",
      "287 [D loss: (-0.072)(R -0.397, F 0.254)]  [G loss: -0.169] \n",
      "288 [D loss: (-0.086)(R -0.420, F 0.248)]  [G loss: -0.153] \n",
      "289 [D loss: (-0.101)(R -0.381, F 0.179)]  [G loss: -0.150] \n",
      "290 [D loss: (-0.083)(R -0.383, F 0.217)]  [G loss: -0.141] \n",
      "291 [D loss: (-0.070)(R -0.386, F 0.245)]  [G loss: -0.174] \n",
      "292 [D loss: (-0.058)(R -0.370, F 0.253)]  [G loss: -0.168] \n",
      "293 [D loss: (-0.048)(R -0.376, F 0.280)]  [G loss: -0.187] \n",
      "294 [D loss: (-0.075)(R -0.395, F 0.244)]  [G loss: -0.173] \n",
      "295 [D loss: (-0.082)(R -0.406, F 0.241)]  [G loss: -0.175] \n",
      "296 [D loss: (-0.075)(R -0.400, F 0.250)]  [G loss: -0.170] \n",
      "297 [D loss: (-0.079)(R -0.373, F 0.216)]  [G loss: -0.151] \n",
      "298 [D loss: (-0.056)(R -0.362, F 0.249)]  [G loss: -0.185] \n",
      "299 [D loss: (-0.062)(R -0.408, F 0.283)]  [G loss: -0.180] \n",
      "300 [D loss: (-0.059)(R -0.376, F 0.258)]  [G loss: -0.181] \n",
      "301 [D loss: (-0.087)(R -0.402, F 0.228)]  [G loss: -0.181] \n",
      "302 [D loss: (-0.084)(R -0.399, F 0.231)]  [G loss: -0.197] \n",
      "303 [D loss: (-0.095)(R -0.436, F 0.246)]  [G loss: -0.183] \n",
      "304 [D loss: (-0.039)(R -0.385, F 0.307)]  [G loss: -0.194] \n",
      "305 [D loss: (-0.086)(R -0.434, F 0.261)]  [G loss: -0.156] \n",
      "306 [D loss: (-0.060)(R -0.370, F 0.250)]  [G loss: -0.166] \n",
      "307 [D loss: (-0.072)(R -0.420, F 0.277)]  [G loss: -0.184] \n",
      "308 [D loss: (-0.074)(R -0.389, F 0.240)]  [G loss: -0.178] \n",
      "309 [D loss: (-0.063)(R -0.409, F 0.283)]  [G loss: -0.194] \n",
      "310 [D loss: (-0.056)(R -0.397, F 0.285)]  [G loss: -0.197] \n",
      "311 [D loss: (-0.071)(R -0.430, F 0.287)]  [G loss: -0.176] \n",
      "312 [D loss: (-0.081)(R -0.433, F 0.272)]  [G loss: -0.196] \n",
      "313 [D loss: (-0.089)(R -0.425, F 0.246)]  [G loss: -0.191] \n",
      "314 [D loss: (-0.060)(R -0.421, F 0.301)]  [G loss: -0.211] \n",
      "315 [D loss: (-0.071)(R -0.421, F 0.279)]  [G loss: -0.180] \n",
      "316 [D loss: (-0.066)(R -0.405, F 0.272)]  [G loss: -0.193] \n",
      "317 [D loss: (-0.071)(R -0.440, F 0.298)]  [G loss: -0.202] \n",
      "318 [D loss: (-0.086)(R -0.447, F 0.275)]  [G loss: -0.226] \n",
      "319 [D loss: (-0.037)(R -0.381, F 0.306)]  [G loss: -0.183] \n",
      "320 [D loss: (-0.071)(R -0.451, F 0.309)]  [G loss: -0.208] \n",
      "321 [D loss: (-0.072)(R -0.447, F 0.303)]  [G loss: -0.238] \n",
      "322 [D loss: (-0.067)(R -0.441, F 0.307)]  [G loss: -0.206] \n",
      "323 [D loss: (-0.054)(R -0.405, F 0.296)]  [G loss: -0.192] \n",
      "324 [D loss: (-0.056)(R -0.399, F 0.287)]  [G loss: -0.195] \n",
      "325 [D loss: (-0.054)(R -0.415, F 0.307)]  [G loss: -0.207] \n",
      "326 [D loss: (-0.081)(R -0.421, F 0.259)]  [G loss: -0.186] \n",
      "327 [D loss: (-0.080)(R -0.452, F 0.292)]  [G loss: -0.193] \n",
      "328 [D loss: (-0.055)(R -0.431, F 0.321)]  [G loss: -0.223] \n",
      "329 [D loss: (-0.085)(R -0.474, F 0.304)]  [G loss: -0.194] \n",
      "330 [D loss: (-0.080)(R -0.473, F 0.313)]  [G loss: -0.221] \n",
      "331 [D loss: (-0.077)(R -0.452, F 0.297)]  [G loss: -0.205] \n",
      "332 [D loss: (-0.075)(R -0.441, F 0.291)]  [G loss: -0.193] \n",
      "333 [D loss: (-0.067)(R -0.422, F 0.288)]  [G loss: -0.212] \n",
      "334 [D loss: (-0.087)(R -0.451, F 0.277)]  [G loss: -0.199] \n",
      "335 [D loss: (-0.086)(R -0.463, F 0.292)]  [G loss: -0.181] \n",
      "336 [D loss: (-0.071)(R -0.463, F 0.320)]  [G loss: -0.219] \n",
      "337 [D loss: (-0.075)(R -0.439, F 0.289)]  [G loss: -0.192] \n",
      "338 [D loss: (-0.110)(R -0.483, F 0.262)]  [G loss: -0.202] \n",
      "339 [D loss: (-0.073)(R -0.447, F 0.301)]  [G loss: -0.193] \n",
      "340 [D loss: (-0.092)(R -0.483, F 0.299)]  [G loss: -0.202] \n",
      "341 [D loss: (-0.082)(R -0.465, F 0.300)]  [G loss: -0.212] \n",
      "342 [D loss: (-0.076)(R -0.446, F 0.295)]  [G loss: -0.193] \n",
      "343 [D loss: (-0.114)(R -0.492, F 0.265)]  [G loss: -0.190] \n",
      "344 [D loss: (-0.086)(R -0.484, F 0.313)]  [G loss: -0.198] \n",
      "345 [D loss: (-0.092)(R -0.495, F 0.311)]  [G loss: -0.193] \n",
      "346 [D loss: (-0.079)(R -0.456, F 0.298)]  [G loss: -0.201] \n",
      "347 [D loss: (-0.077)(R -0.453, F 0.299)]  [G loss: -0.202] \n",
      "348 [D loss: (-0.063)(R -0.466, F 0.340)]  [G loss: -0.220] \n",
      "349 [D loss: (-0.069)(R -0.456, F 0.318)]  [G loss: -0.197] \n",
      "350 [D loss: (-0.092)(R -0.488, F 0.303)]  [G loss: -0.211] \n",
      "351 [D loss: (-0.081)(R -0.461, F 0.298)]  [G loss: -0.210] \n",
      "352 [D loss: (-0.101)(R -0.483, F 0.280)]  [G loss: -0.203] \n",
      "353 [D loss: (-0.067)(R -0.477, F 0.344)]  [G loss: -0.198] \n",
      "354 [D loss: (-0.120)(R -0.506, F 0.266)]  [G loss: -0.209] \n",
      "355 [D loss: (-0.084)(R -0.471, F 0.302)]  [G loss: -0.214] \n",
      "356 [D loss: (-0.107)(R -0.501, F 0.286)]  [G loss: -0.219] \n",
      "357 [D loss: (-0.095)(R -0.506, F 0.316)]  [G loss: -0.210] \n",
      "358 [D loss: (-0.086)(R -0.442, F 0.271)]  [G loss: -0.205] \n",
      "359 [D loss: (-0.096)(R -0.477, F 0.285)]  [G loss: -0.204] \n",
      "360 [D loss: (-0.107)(R -0.534, F 0.320)]  [G loss: -0.225] \n",
      "361 [D loss: (-0.085)(R -0.503, F 0.333)]  [G loss: -0.238] \n",
      "362 [D loss: (-0.094)(R -0.495, F 0.307)]  [G loss: -0.241] \n",
      "363 [D loss: (-0.101)(R -0.525, F 0.324)]  [G loss: -0.248] \n",
      "364 [D loss: (-0.085)(R -0.491, F 0.321)]  [G loss: -0.231] \n",
      "365 [D loss: (-0.098)(R -0.507, F 0.312)]  [G loss: -0.232] \n",
      "366 [D loss: (-0.093)(R -0.489, F 0.302)]  [G loss: -0.243] \n",
      "367 [D loss: (-0.102)(R -0.556, F 0.352)]  [G loss: -0.261] \n",
      "368 [D loss: (-0.091)(R -0.503, F 0.322)]  [G loss: -0.224] \n",
      "369 [D loss: (-0.111)(R -0.553, F 0.332)]  [G loss: -0.269] \n",
      "370 [D loss: (-0.103)(R -0.546, F 0.340)]  [G loss: -0.248] \n",
      "371 [D loss: (-0.101)(R -0.518, F 0.316)]  [G loss: -0.246] \n",
      "372 [D loss: (-0.061)(R -0.498, F 0.375)]  [G loss: -0.248] \n",
      "373 [D loss: (-0.060)(R -0.499, F 0.380)]  [G loss: -0.268] \n",
      "374 [D loss: (-0.092)(R -0.549, F 0.364)]  [G loss: -0.261] \n",
      "375 [D loss: (-0.100)(R -0.567, F 0.367)]  [G loss: -0.241] \n",
      "376 [D loss: (-0.098)(R -0.538, F 0.343)]  [G loss: -0.264] \n",
      "377 [D loss: (-0.071)(R -0.524, F 0.382)]  [G loss: -0.281] \n",
      "378 [D loss: (-0.070)(R -0.527, F 0.388)]  [G loss: -0.260] \n",
      "379 [D loss: (-0.105)(R -0.578, F 0.368)]  [G loss: -0.247] \n",
      "380 [D loss: (-0.098)(R -0.546, F 0.350)]  [G loss: -0.275] \n",
      "381 [D loss: (-0.121)(R -0.597, F 0.355)]  [G loss: -0.268] \n",
      "382 [D loss: (-0.095)(R -0.554, F 0.363)]  [G loss: -0.242] \n",
      "383 [D loss: (-0.135)(R -0.571, F 0.302)]  [G loss: -0.266] \n",
      "384 [D loss: (-0.069)(R -0.510, F 0.371)]  [G loss: -0.257] \n",
      "385 [D loss: (-0.109)(R -0.556, F 0.338)]  [G loss: -0.260] \n",
      "386 [D loss: (-0.115)(R -0.570, F 0.341)]  [G loss: -0.256] \n",
      "387 [D loss: (-0.078)(R -0.526, F 0.371)]  [G loss: -0.271] \n",
      "388 [D loss: (-0.101)(R -0.575, F 0.373)]  [G loss: -0.237] \n",
      "389 [D loss: (-0.093)(R -0.524, F 0.338)]  [G loss: -0.249] \n",
      "390 [D loss: (-0.089)(R -0.494, F 0.316)]  [G loss: -0.234] \n",
      "391 [D loss: (-0.068)(R -0.523, F 0.386)]  [G loss: -0.270] \n",
      "392 [D loss: (-0.137)(R -0.576, F 0.303)]  [G loss: -0.233] \n",
      "393 [D loss: (-0.092)(R -0.507, F 0.324)]  [G loss: -0.188] \n",
      "394 [D loss: (-0.086)(R -0.531, F 0.360)]  [G loss: -0.262] \n",
      "395 [D loss: (-0.102)(R -0.588, F 0.384)]  [G loss: -0.264] \n",
      "396 [D loss: (-0.073)(R -0.518, F 0.373)]  [G loss: -0.255] \n",
      "397 [D loss: (-0.091)(R -0.544, F 0.362)]  [G loss: -0.238] \n",
      "398 [D loss: (-0.104)(R -0.543, F 0.336)]  [G loss: -0.260] \n",
      "399 [D loss: (-0.122)(R -0.554, F 0.310)]  [G loss: -0.212] \n",
      "400 [D loss: (-0.081)(R -0.517, F 0.354)]  [G loss: -0.201] \n",
      "401 [D loss: (-0.104)(R -0.559, F 0.351)]  [G loss: -0.249] \n",
      "402 [D loss: (-0.110)(R -0.572, F 0.352)]  [G loss: -0.244] \n",
      "403 [D loss: (-0.129)(R -0.583, F 0.326)]  [G loss: -0.247] \n",
      "404 [D loss: (-0.091)(R -0.530, F 0.348)]  [G loss: -0.232] \n",
      "405 [D loss: (-0.098)(R -0.542, F 0.346)]  [G loss: -0.216] \n",
      "406 [D loss: (-0.101)(R -0.554, F 0.352)]  [G loss: -0.218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407 [D loss: (-0.105)(R -0.533, F 0.323)]  [G loss: -0.215] \n",
      "408 [D loss: (-0.086)(R -0.502, F 0.331)]  [G loss: -0.229] \n",
      "409 [D loss: (-0.109)(R -0.512, F 0.295)]  [G loss: -0.204] \n",
      "410 [D loss: (-0.094)(R -0.504, F 0.315)]  [G loss: -0.198] \n",
      "411 [D loss: (-0.099)(R -0.515, F 0.317)]  [G loss: -0.205] \n",
      "412 [D loss: (-0.106)(R -0.540, F 0.328)]  [G loss: -0.218] \n",
      "413 [D loss: (-0.047)(R -0.443, F 0.349)]  [G loss: -0.258] \n",
      "414 [D loss: (-0.108)(R -0.534, F 0.318)]  [G loss: -0.223] \n",
      "415 [D loss: (-0.094)(R -0.523, F 0.335)]  [G loss: -0.215] \n",
      "416 [D loss: (-0.075)(R -0.461, F 0.311)]  [G loss: -0.240] \n",
      "417 [D loss: (-0.101)(R -0.519, F 0.317)]  [G loss: -0.230] \n",
      "418 [D loss: (-0.082)(R -0.482, F 0.318)]  [G loss: -0.221] \n",
      "419 [D loss: (-0.102)(R -0.536, F 0.332)]  [G loss: -0.217] \n",
      "420 [D loss: (-0.066)(R -0.477, F 0.346)]  [G loss: -0.209] \n",
      "421 [D loss: (-0.062)(R -0.469, F 0.344)]  [G loss: -0.203] \n",
      "422 [D loss: (-0.079)(R -0.479, F 0.321)]  [G loss: -0.201] \n",
      "423 [D loss: (-0.113)(R -0.498, F 0.272)]  [G loss: -0.191] \n",
      "424 [D loss: (-0.121)(R -0.538, F 0.295)]  [G loss: -0.228] \n",
      "425 [D loss: (-0.095)(R -0.493, F 0.303)]  [G loss: -0.227] \n",
      "426 [D loss: (-0.066)(R -0.457, F 0.324)]  [G loss: -0.202] \n",
      "427 [D loss: (-0.108)(R -0.540, F 0.325)]  [G loss: -0.211] \n",
      "428 [D loss: (-0.118)(R -0.539, F 0.303)]  [G loss: -0.196] \n",
      "429 [D loss: (-0.137)(R -0.541, F 0.266)]  [G loss: -0.187] \n",
      "430 [D loss: (-0.091)(R -0.528, F 0.347)]  [G loss: -0.222] \n",
      "431 [D loss: (-0.084)(R -0.475, F 0.307)]  [G loss: -0.209] \n",
      "432 [D loss: (-0.106)(R -0.527, F 0.315)]  [G loss: -0.201] \n",
      "433 [D loss: (-0.073)(R -0.509, F 0.363)]  [G loss: -0.227] \n",
      "434 [D loss: (-0.096)(R -0.533, F 0.341)]  [G loss: -0.213] \n",
      "435 [D loss: (-0.082)(R -0.503, F 0.339)]  [G loss: -0.212] \n",
      "436 [D loss: (-0.094)(R -0.506, F 0.319)]  [G loss: -0.186] \n",
      "437 [D loss: (-0.115)(R -0.518, F 0.288)]  [G loss: -0.185] \n",
      "438 [D loss: (-0.075)(R -0.517, F 0.368)]  [G loss: -0.231] \n",
      "439 [D loss: (-0.090)(R -0.489, F 0.308)]  [G loss: -0.214] \n",
      "440 [D loss: (-0.091)(R -0.527, F 0.346)]  [G loss: -0.224] \n",
      "441 [D loss: (-0.093)(R -0.535, F 0.349)]  [G loss: -0.227] \n",
      "442 [D loss: (-0.102)(R -0.536, F 0.332)]  [G loss: -0.209] \n",
      "443 [D loss: (-0.110)(R -0.516, F 0.295)]  [G loss: -0.182] \n",
      "444 [D loss: (-0.108)(R -0.544, F 0.329)]  [G loss: -0.249] \n",
      "445 [D loss: (-0.118)(R -0.591, F 0.355)]  [G loss: -0.227] \n",
      "446 [D loss: (-0.078)(R -0.507, F 0.350)]  [G loss: -0.252] \n",
      "447 [D loss: (-0.084)(R -0.528, F 0.361)]  [G loss: -0.221] \n",
      "448 [D loss: (-0.074)(R -0.498, F 0.349)]  [G loss: -0.238] \n",
      "449 [D loss: (-0.105)(R -0.529, F 0.318)]  [G loss: -0.192] \n",
      "450 [D loss: (-0.098)(R -0.532, F 0.335)]  [G loss: -0.202] \n",
      "451 [D loss: (-0.128)(R -0.580, F 0.325)]  [G loss: -0.207] \n",
      "452 [D loss: (-0.094)(R -0.509, F 0.322)]  [G loss: -0.187] \n",
      "453 [D loss: (-0.099)(R -0.531, F 0.333)]  [G loss: -0.226] \n",
      "454 [D loss: (-0.069)(R -0.480, F 0.341)]  [G loss: -0.206] \n",
      "455 [D loss: (-0.087)(R -0.502, F 0.328)]  [G loss: -0.208] \n",
      "456 [D loss: (-0.125)(R -0.553, F 0.303)]  [G loss: -0.173] \n",
      "457 [D loss: (-0.094)(R -0.474, F 0.286)]  [G loss: -0.159] \n",
      "458 [D loss: (-0.102)(R -0.520, F 0.316)]  [G loss: -0.222] \n",
      "459 [D loss: (-0.138)(R -0.515, F 0.238)]  [G loss: -0.144] \n",
      "460 [D loss: (-0.118)(R -0.527, F 0.290)]  [G loss: -0.180] \n",
      "461 [D loss: (-0.086)(R -0.456, F 0.285)]  [G loss: -0.208] \n",
      "462 [D loss: (-0.066)(R -0.481, F 0.349)]  [G loss: -0.202] \n",
      "463 [D loss: (-0.115)(R -0.576, F 0.346)]  [G loss: -0.221] \n",
      "464 [D loss: (-0.106)(R -0.514, F 0.302)]  [G loss: -0.203] \n",
      "465 [D loss: (-0.076)(R -0.505, F 0.353)]  [G loss: -0.200] \n",
      "466 [D loss: (-0.107)(R -0.527, F 0.312)]  [G loss: -0.184] \n",
      "467 [D loss: (-0.130)(R -0.525, F 0.265)]  [G loss: -0.227] \n",
      "468 [D loss: (-0.095)(R -0.514, F 0.324)]  [G loss: -0.187] \n",
      "469 [D loss: (-0.138)(R -0.530, F 0.255)]  [G loss: -0.187] \n",
      "470 [D loss: (-0.091)(R -0.523, F 0.340)]  [G loss: -0.190] \n",
      "471 [D loss: (-0.079)(R -0.506, F 0.348)]  [G loss: -0.187] \n",
      "472 [D loss: (-0.096)(R -0.523, F 0.331)]  [G loss: -0.176] \n",
      "473 [D loss: (-0.084)(R -0.505, F 0.337)]  [G loss: -0.187] \n",
      "474 [D loss: (-0.085)(R -0.501, F 0.332)]  [G loss: -0.187] \n",
      "475 [D loss: (-0.078)(R -0.473, F 0.317)]  [G loss: -0.193] \n",
      "476 [D loss: (-0.112)(R -0.502, F 0.279)]  [G loss: -0.156] \n",
      "477 [D loss: (-0.131)(R -0.472, F 0.211)]  [G loss: -0.150] \n",
      "478 [D loss: (-0.083)(R -0.508, F 0.341)]  [G loss: -0.190] \n",
      "479 [D loss: (-0.109)(R -0.524, F 0.305)]  [G loss: -0.205] \n",
      "480 [D loss: (-0.098)(R -0.463, F 0.266)]  [G loss: -0.167] \n",
      "481 [D loss: (-0.102)(R -0.534, F 0.330)]  [G loss: -0.175] \n",
      "482 [D loss: (-0.087)(R -0.511, F 0.337)]  [G loss: -0.177] \n",
      "483 [D loss: (-0.076)(R -0.477, F 0.325)]  [G loss: -0.167] \n",
      "484 [D loss: (-0.098)(R -0.513, F 0.317)]  [G loss: -0.183] \n",
      "485 [D loss: (-0.090)(R -0.457, F 0.276)]  [G loss: -0.173] \n",
      "486 [D loss: (-0.111)(R -0.485, F 0.262)]  [G loss: -0.153] \n",
      "487 [D loss: (-0.108)(R -0.458, F 0.243)]  [G loss: -0.191] \n",
      "488 [D loss: (-0.125)(R -0.477, F 0.226)]  [G loss: -0.129] \n",
      "489 [D loss: (-0.091)(R -0.437, F 0.255)]  [G loss: -0.180] \n",
      "490 [D loss: (-0.088)(R -0.472, F 0.295)]  [G loss: -0.150] \n",
      "491 [D loss: (-0.094)(R -0.490, F 0.302)]  [G loss: -0.150] \n",
      "492 [D loss: (-0.119)(R -0.469, F 0.230)]  [G loss: -0.128] \n",
      "493 [D loss: (-0.084)(R -0.439, F 0.272)]  [G loss: -0.152] \n",
      "494 [D loss: (-0.112)(R -0.453, F 0.228)]  [G loss: -0.119] \n",
      "495 [D loss: (-0.087)(R -0.476, F 0.302)]  [G loss: -0.146] \n",
      "496 [D loss: (-0.132)(R -0.506, F 0.242)]  [G loss: -0.168] \n",
      "497 [D loss: (-0.092)(R -0.441, F 0.257)]  [G loss: -0.093] \n",
      "498 [D loss: (-0.117)(R -0.479, F 0.245)]  [G loss: -0.148] \n",
      "499 [D loss: (-0.121)(R -0.518, F 0.277)]  [G loss: -0.136] \n"
     ]
    }
   ],
   "source": [
    "wgan.train(     \n",
    "    x\n",
    "    , batch_size = 64\n",
    "    , epochs = 500\n",
    "    , print_every_n_batches = 5\n",
    "    , n_critic = 5\n",
    "    , clip_threshold = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
